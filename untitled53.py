# -*- coding: utf-8 -*-
"""Untitled53.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yeMWjGeZu6U7QzckOW_P7MLWSbkPO1LQ
"""

with open('/content/disease_qa.jsonl', 'r') as f:
    for line in f:
        print(line)

import json

data = [
    {"question": "What are the symptoms of diabetes?", "answer": "Frequent urination, thirst, fatigue, and blurred vision."},
    {"question": "How can hypertension be managed?", "answer": "Reduce salt, exercise, manage stress, and take prescribed medication."},
    {"question": "What triggers asthma attacks?", "answer": "Allergens, cold air, exercise, infections, and air pollution."},
    {"question": "How is anemia treated?", "answer": "With iron supplements, B12/folic acid, diet changes, and treating causes."}
]

with open("/content/disease_qa.jsonl", "w") as f:
    for item in data:
        f.write(json.dumps(item) + "\n")

!pip install datasets

from datasets import load_dataset

dataset = load_dataset("json", data_files={"train": "/content/disease_qa.jsonl"})
print(dataset)
print(dataset['train'][0])

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize_function(example):
    return tokenizer(
        example["question"],
        example["answer"],
        truncation=True,
        padding="max_length",
        max_length=256
    )

tokenized_datasets = dataset.map(tokenize_function, batched=True)

!pip install --upgrade transformers

!pip install transformers datasets

import json

# Load your disease QA dataset into a list
with open('/content/disease_qa.jsonl') as f:
    qa_data = [json.loads(line) for line in f]

prompt = 'HealthBot: Ask me about diabetes, hypertension, or asthma! (type \'exit\' to quit)'
print(prompt)

while True:
    user_input = input("You: ")
    if user_input.lower() == "exit":
        break

    # Search for a matching question
    matched_answer = None
    for item in qa_data:
        if user_input.lower() in item["question"].lower():
            matched_answer = item["answer"]
            break

    if matched_answer:
        print("HealthBot:", matched_answer)
    else:
        # Fall back to LLaMA model if no match
        full_prompt = f"{prompt} You: {user_input} HealthBot:"
        response = qa_pipeline(full_prompt, max_new_tokens=50)
        answer = response[0]['generated_text'].split("HealthBot:")[-1].strip()
        print("HealthBot:", answer)